{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys, imp, math, random, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random, copy\n",
    "from pathlib import Path\n",
    "\n",
    "from io import StringIO\n",
    "from sklearn import tree, metrics\n",
    "from sklearn import feature_selection\n",
    "import sklearn\n",
    "from downstream_model import *\n",
    "from Featurize import *\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from random import choices\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "plt.rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Directory to save log files from the simulation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curWD = 'logs/synthetic/allx/duplicates-1(dr=3)' \n",
    "Path(curWD).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problst = []\n",
    "CPTdic = {}\n",
    "\n",
    "seed = 100\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "numD = 100 # number of clean datasets\n",
    "dr = 3 # number of categorical features\n",
    "dxrlst = [10] # domain size of all categorical features\n",
    "trE = 5000 # number of training examples\n",
    "nr = int(5*trE/3) # total number of examples is determined from given trE\n",
    "dupColIndex = 0 # the duplicate column index, present to 0\n",
    "\n",
    "skewPresent = 1 # if skew not present in duplication parameters, then only specify the below parameters.\n",
    "PERC_ENTY = 30 # fraction of entities that are diluted with dups\n",
    "PERC_OCCUR = 25 # total occurrence value of the duplicate set\n",
    "GRP_SIZE_MINUS_1 = 1 # duplicate set size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writecsvfile(fname,data):\n",
    "    mycsv = csv.writer(open(fname,'wb'), quoting=csv.QUOTE_ALL)\n",
    "    for row in data:\n",
    "        mycsv.writerow(row)\n",
    "\n",
    "def removekey(d, key):\n",
    "    r = dict(d)\n",
    "    del r[key]\n",
    "    return r\n",
    "\n",
    "def onehotvector(x,k):\n",
    "    lst = [0] * k\n",
    "    lst[x] = 1\n",
    "    return lst\n",
    "\n",
    "\n",
    "def chunkIt(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out\n",
    "\n",
    "def createCPT(DomainSize):\n",
    "    RandNumLst = []\n",
    "    for i in range(DomainSize):\n",
    "        randNum = random.uniform(0, 1)\n",
    "        RandNumLst.append(randNum)\n",
    "    return RandNumLst\n",
    "\n",
    "def createCPTGamma(DomainSize):\n",
    "    p = 0.9\n",
    "    gamma = 0.99\n",
    "    \n",
    "    RandNumLst = []\n",
    "    for i in range(DomainSize):\n",
    "        randNum = p * (gamma ** i)\n",
    "        RandNumLst.append(randNum)\n",
    "    return RandNumLst\n",
    "\n",
    "def createCPTBeta(DomainSize):\n",
    "    RandNumLst = []\n",
    "    for i in range(DomainSize):\n",
    "        randNum = np.random.beta(5,2)\n",
    "        RandNumLst.append(randNum)\n",
    "    return RandNumLst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SampleZipf(nr, dxr, NumEnts2sample):\n",
    "    a = 2\n",
    "    s = np.random.zipf(a, nr)\n",
    "    \n",
    "    vals = dxr\n",
    "    count, bins, ignored = plt.hist(s[s<vals], vals, density=True)\n",
    "    plt.close()\n",
    "    \n",
    "    count_lst = count.tolist()\n",
    "    \n",
    "    sum_freq = sum(count_lst)\n",
    "\n",
    "    count_lst = [x*1.0/sum_freq for x in count_lst]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.bar([i for i in range(1,dxr+1)], count_lst)\n",
    "    plt.xticks([i for i in range(1,dxr+1)])\n",
    "#     plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    tmp = np.random.choice(a = dxr, size = NumEnts2sample, p = count_lst)\n",
    "    sampled_vals = [x+1 for x in tmp]\n",
    "\n",
    "    \n",
    "    return sampled_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateNDCPT(n,dxr):\n",
    "    \n",
    "    global dr\n",
    "    cptdic = {}\n",
    "\n",
    "    if dr == 3:\n",
    "    \n",
    "        j=0\n",
    "        while j < dxr:\n",
    "            for k in range(dxr):\n",
    "                tmpstr = ''\n",
    "                for l in range(dxr):\n",
    "                    tmpstr = str(j) + '#'\n",
    "                    tmpstr = tmpstr + str(k) + '#'                    \n",
    "                    tmpstr = tmpstr + str(l) + '#'\n",
    "                    randNum = random.uniform(0, 1)\n",
    "                    if randNum > 0.5: randNum = 1\n",
    "                    else: randNum = 0\n",
    "                    cptdic[tmpstr] = randNum\n",
    "            j += 1    \n",
    "      \n",
    "    return cptdic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AblationStudy(dataDownstream, y, y_cur, attribute_names, scrtr_LR, scrval_LR, scrte_LR, scrtr_RF, scrval_RF, scrte_RF):\n",
    "\n",
    "    dataDownstreamX = copy.deepcopy(dataDownstream)   \n",
    "\n",
    "    truthModel_LR, truth_train_LR, truth_val_LR, truth_test_LR = 'lrm', scrtr_LR, scrval_LR, scrte_LR\n",
    "    truthModel_RF, truth_train_RF, truth_val_RF, truth_test_RF = 'rfm', scrtr_RF, scrval_RF, scrte_RF\n",
    "\n",
    "    print(truth_train_LR, truth_val_LR, truth_test_LR)\n",
    "    print(truth_train_RF, truth_val_RF, truth_test_RF)\n",
    "\n",
    "    dataDownstream_dedup_ablation = copy.deepcopy(dataDownstreamX)\n",
    "\n",
    "    attribute_names_ablation = []\n",
    "    y_cur_ablation = []\n",
    "\n",
    "    for i in range(len(attribute_names)):\n",
    "        if y_cur[i] != 7:\n",
    "            attribute_names_ablation.append(attribute_names[i])\n",
    "            y_cur_ablation.append(y_cur[i])\n",
    "\n",
    "    print(attribute_names_ablation)\n",
    "    print(y_cur_ablation)\n",
    "\n",
    "    dataDownstream_dedup_ablation = dataDownstream_dedup_ablation[attribute_names_ablation]\n",
    "\n",
    "    models_LR, trainscores_LR, valscores_LR, testscores_LR = [],[],[],[]\n",
    "    models_RF, trainscores_RF, valscores_RF, testscores_RF = [],[],[],[]\n",
    "\n",
    "    for i in range(len(y_cur_ablation)):\n",
    "        print(y_cur_ablation[i])\n",
    "\n",
    "        attribute_names_moving, y_cur_moving = copy.deepcopy(attribute_names_ablation),copy.deepcopy(y_cur_ablation)\n",
    "        curcol = attribute_names_moving[i]\n",
    "        curdf = dataDownstream_dedup_ablation.drop(curcol, axis = 1)\n",
    "\n",
    "        attribute_names_moving.pop(i)\n",
    "        y_cur_moving.pop(i)\n",
    "\n",
    "        print('***')\n",
    "        print(curcol)\n",
    "        print(attribute_names_moving)\n",
    "        ########################################\n",
    "        bestPerformingModel_LR, avgsc_train_lst_LR, avgsc_lst_LR, avgsc_hld_lst_LR = LogRegClassifier(curdf,y, y_cur_moving,attribute_names_moving,0)\n",
    "        bestPerformingModel_RF, avgsc_train_lst_RF, avgsc_lst_RF, avgsc_hld_lst_RF = RandForestClassifier(curdf,y, y_cur_moving,attribute_names_moving,0)\n",
    "        ########################################\n",
    "        trainscores_LR.append(np.mean(avgsc_train_lst_LR))\n",
    "        valscores_LR.append(np.mean(avgsc_lst_LR))\n",
    "        testscores_LR.append(np.mean(avgsc_hld_lst_LR))\n",
    "        ########################################\n",
    "        trainscores_RF.append(np.mean(avgsc_train_lst_RF))\n",
    "        valscores_RF.append(np.mean(avgsc_lst_RF))\n",
    "        testscores_RF.append(np.mean(avgsc_hld_lst_RF))\n",
    "        ########################################    \n",
    "    \n",
    "    print(trainscores_LR, valscores_LR, testscores_LR)\n",
    "    print(trainscores_RF, valscores_RF, testscores_RF)\n",
    "    \n",
    "    drops_by_attribute_LR,drops_by_attribute_RF,drops_by_attribute_h2o = [],[],[]\n",
    "    for i in range(len(attribute_names_ablation)):\n",
    "        print(attribute_names_ablation[i])\n",
    "\n",
    "        drop_LR = (truth_test_LR - testscores_LR[i]*100)\n",
    "        print(drop_LR)\n",
    "        drops_by_attribute_LR.append(drop_LR)\n",
    "\n",
    "        drop_RF = (truth_test_RF - testscores_RF[i]*100)\n",
    "        print(drop_RF)\n",
    "        drops_by_attribute_RF.append(drop_RF) \n",
    "\n",
    "        print('\\n')    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['attribute_names'] = attribute_names_ablation\n",
    "    df['drop_LR'] = drops_by_attribute_LR\n",
    "    df['drop_RF'] = drops_by_attribute_RF\n",
    "    \n",
    "    LR_index = (np.argsort(-np.array(drops_by_attribute_LR))+1).tolist()\n",
    "    RF_index = (np.argsort(-np.array(drops_by_attribute_RF))+1).tolist()    \n",
    "    \n",
    "    return df,LR_index,RF_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getY(rtup_lst,dxr):\n",
    "    global CPTdic, dr\n",
    "    \n",
    "    tmpstr = ''\n",
    "    for j in range(len(rtup_lst)):\n",
    "#         if j == dr-1: break\n",
    "        tmpstr = tmpstr + str(int(rtup_lst[j])) + '#'    \n",
    "    \n",
    "    prob0 = CPTdic[tmpstr]\n",
    "    \n",
    "    thisy = np.random.choice([0,1],1,p=[prob0,1-prob0]).tolist()[0]\n",
    "    return thisy\n",
    "\n",
    "def buildDataset(trainsetx, trainsetx_Number, trainsetx_both, trainsety, rtup_lst, DomainLst,OHElst,dxr):\n",
    "    global ProbLst, dr\n",
    "    \n",
    "    dval = copy.deepcopy(rtup_lst)\n",
    "\n",
    "    thisy = getY(rtup_lst,dxr)\n",
    "    dval_Number = dval\n",
    "    \n",
    "    trainsetx_Number = trainsetx_Number + [dval_Number]\n",
    "    trainsety = trainsety + [thisy]\n",
    "    \n",
    "    return 'trainsetx', trainsetx_Number, '', trainsety\n",
    "\n",
    "def DatasetToDataFrame(trainsetx, trainsety):\n",
    "    global dupColIndex\n",
    "    dataDownstream = pd.DataFrame(trainsetx)\n",
    "    dataDownstream.rename(columns={ dataDownstream.columns[dupColIndex]: \"duplicateColumn\" }, inplace = True)\n",
    "\n",
    "    attribute_names = dataDownstream.columns.tolist()\n",
    "\n",
    "    if dupColIndex == 0:\n",
    "        y_cur = [10]\n",
    "        for x in range(len(attribute_names)-1): y_cur.append(1)\n",
    "\n",
    "    if dupColIndex == 2:            \n",
    "        y_cur = []\n",
    "        for x in range(len(attribute_names)-1): y_cur.append(1)\n",
    "        y_cur.append(10)\n",
    "        \n",
    "    if dupColIndex == 3:            \n",
    "        y_cur = [1,1,1,10]\n",
    "        \n",
    "    y =  pd.DataFrame(trainsety, columns = ['y'])\n",
    "\n",
    "    return dataDownstream, y, y_cur, attribute_names\n",
    "\n",
    "\n",
    "def IntroduceDirtiness(dfAblation, dataDownstream, attribute_names, dxr, PERC_OCCUR, scrtr_LR, scrval_LR, scrte_LR, scrtr_RF, scrval_RF, scrte_RF):\n",
    "    global SAVEPLACE, PERC_ENTY\n",
    "    categcols = ['duplicateColumn']\n",
    "    \n",
    "    ### p1_perc --> Percentage of entities that have duplicates\n",
    "    ### noise --> Percentage of occurences that are diluted with duplicate values    \n",
    "    \n",
    "    GRP_SIZE_MINUS_1 = 1\n",
    "    PERC_OCCUR = PERC_OCCUR*0.01\n",
    "    NUM_DIRTY_DT = 10\n",
    "\n",
    "    for curcol in categcols:\n",
    "        curdic = dict(dataDownstream[curcol].value_counts())\n",
    "\n",
    "        p1_perc = [PERC_ENTY]\n",
    "        for CURPERC in p1_perc:\n",
    "            print('CURPERC value is:' + str(CURPERC))\n",
    "            \n",
    "            tmp = ((dataDownstream[curcol].nunique())*CURPERC)*1.0/100\n",
    "            NENT = int(round(tmp,0))            \n",
    "\n",
    "            lst_vals = list(curdic.keys())\n",
    "            \n",
    "            print('Dictionary keys:')\n",
    "            print(lst_vals)            \n",
    "            \n",
    "            if CURPERC == 100:\n",
    "                NENT = dataDownstream[curcol].nunique()\n",
    "                possible_combinations = [random.sample(lst_vals,NENT)]\n",
    "            else:\n",
    "                possible_combinations = []\n",
    "                dic_of_strlsts = {}\n",
    "                \n",
    "                while True:\n",
    "                    if len(dic_of_strlsts) == NUM_DIRTY_DT: break\n",
    "                    tmplst = random.sample(lst_vals, NENT)\n",
    "\n",
    "                    if str(sorted(tmplst)) not in dic_of_strlsts:\n",
    "                        dic_of_strlsts[str(sorted(tmplst))] = 1\n",
    "                        possible_combinations.append(tmplst)     \n",
    "         \n",
    "                print('All possible_combinations:')\n",
    "                print(possible_combinations)  \n",
    "                   \n",
    "\n",
    "            print('NENT value is:' + str(NENT))\n",
    "\n",
    "            noise = PERC_OCCUR\n",
    "            indexlstlst = []\n",
    "\n",
    "            bestPerformingModel_LR_full, avgsc_train_lst_LR_full, avgsc_lst_LR_full, avgsc_hld_lst_LR_full = [],[],[],[]\n",
    "            bestPerformingModel_RF_full, avgsc_train_lst_RF_full, avgsc_lst_RF_full, avgsc_hld_lst_RF_full = [],[],[],[]\n",
    "\n",
    "            k = 0\n",
    "            for comb in possible_combinations:\n",
    "#                 if k > 1: break\n",
    "                k = k + 1\n",
    "                print(comb)\n",
    "                curdataDownstream = copy.deepcopy(dataDownstream)\n",
    "\n",
    "                for pq in comb:\n",
    "#                     print('\\nCurrent value is:' + str(pq))\n",
    "    \n",
    "                    dirtiness = int(curdic[pq]*noise)\n",
    "#                     print(dirtiness)\n",
    "                    abc = curdataDownstream[curdataDownstream[curcol] == pq].sample(dirtiness, random_state=1)\n",
    "                    indxlst = abc.index.tolist()\n",
    "                    indexlstlst.append(indxlst)\n",
    "\n",
    "                    chk_indxlst = chunkIt(indxlst, GRP_SIZE_MINUS_1)\n",
    "\n",
    "                    for j in range(len(chk_indxlst)): curdataDownstream.loc[chk_indxlst[j], curcol] = str(pq) + '_' + str(j) + '_dummy'    \n",
    "\n",
    "                print(curdataDownstream[curcol].value_counts())\n",
    "                print('\\n')         \n",
    "\n",
    "                bestPerformingModel_LR,avgsc_train_lst_LR,avgsc_lst_LR,avgsc_hld_lst_LR = LogRegClassifier(curdataDownstream, y, y_cur, attribute_names, 0)\n",
    "                bestPerformingModel_RF,avgsc_train_lst_RF,avgsc_lst_RF,avgsc_hld_lst_RF = RandForestClassifier(curdataDownstream, y, y_cur, attribute_names, 0)\n",
    "                RandForestH2oClassifier\n",
    "        \n",
    "                avgsc_train_lst_LR,avgsc_lst_LR,avgsc_hld_lst_LR = round(avgsc_train_lst_LR*100.0,3), round(avgsc_lst_LR*100.0,3), round(avgsc_hld_lst_LR*100.0,3)\n",
    "                avgsc_train_lst_RF,avgsc_lst_RF,avgsc_hld_lst_RF = round(avgsc_train_lst_RF*100.0,3), round(avgsc_lst_RF*100.0,3), round(avgsc_hld_lst_RF*100.0,3)\n",
    "        \n",
    "                bestPerformingModel_LR_full.append(bestPerformingModel_LR)\n",
    "                avgsc_train_lst_LR_full.append(avgsc_train_lst_LR)\n",
    "                avgsc_lst_LR_full.append(avgsc_lst_LR)\n",
    "                avgsc_hld_lst_LR_full.append(avgsc_hld_lst_LR)\n",
    "\n",
    "                bestPerformingModel_RF_full.append(bestPerformingModel_RF)\n",
    "                avgsc_train_lst_RF_full.append(avgsc_train_lst_RF)\n",
    "                avgsc_lst_RF_full.append(avgsc_lst_RF)\n",
    "                avgsc_hld_lst_RF_full.append(avgsc_hld_lst_RF)\n",
    "\n",
    "#             diff_train_lst_LR_full, diff_lst_LR_full, diff_hld_lst_LR_full, diff_train_lst_RF_full, diff_lst_RF_full, diff_hld_lst_RF_full = [], [], [], [], [], []\n",
    "            \n",
    "            diff_train_lst_LR_full = [round(scrtr_LR - number,3) for number in avgsc_train_lst_LR_full]\n",
    "            diff_lst_LR_full = [round(scrval_LR - number,3) for number in avgsc_lst_LR_full] \n",
    "            diff_hld_lst_LR_full = [round(scrte_LR - number,3) for number in avgsc_hld_lst_LR_full]\n",
    "            \n",
    "            diff_train_lst_RF_full = [round(scrtr_RF - number,3) for number in avgsc_train_lst_RF_full]\n",
    "            diff_lst_RF_full = [round(scrval_RF - number,3) for number in avgsc_lst_RF_full]\n",
    "            diff_hld_lst_RF_full = [round(scrte_RF - number,3) for number in avgsc_hld_lst_RF_full]\n",
    "\n",
    "#             curWD = 'logs/synthetic/duplicates-' + str(GRP_SIZE_MINUS_1)\n",
    "#             Path(curWD).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "#             sample = open(curWD + '/' + str(curcol) + '-' + str(CURPERC) + '.txt', 'a')\n",
    "            sample = open(SAVEPLACE, 'a')\n",
    "            print('Original Dataset:', file = sample)\n",
    "            print('---', file = sample)\n",
    "            print(scrtr_LR, file = sample)\n",
    "            print(scrval_LR, file = sample)\n",
    "            print(scrte_LR, file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(scrtr_RF, file = sample)\n",
    "            print(scrval_RF, file = sample)\n",
    "            print(scrte_RF, file = sample)\n",
    "#             print('\\n', file = sample)            \n",
    "            \n",
    "            print('---', file = sample)            \n",
    "            print(dfAblation, file = sample)\n",
    "            print('\\n', file = sample)             \n",
    "            \n",
    "            print('Categories that are diluted with duplicates:', file = sample)\n",
    "            print(possible_combinations[:10], file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(avgsc_train_lst_LR_full, file = sample)\n",
    "            print(avgsc_lst_LR_full, file = sample)\n",
    "            print(avgsc_hld_lst_LR_full, file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(avgsc_train_lst_RF_full, file = sample)\n",
    "            print(avgsc_lst_RF_full, file = sample)\n",
    "            print(avgsc_hld_lst_RF_full, file = sample)\n",
    "            print('\\n', file = sample)\n",
    "            \n",
    "            print('Difference between Orignal and Duplicate Dataset:', file = sample)\n",
    "            print('---', file = sample)\n",
    "            print(diff_train_lst_LR_full, file = sample)\n",
    "            print(diff_lst_LR_full, file = sample)\n",
    "            print(diff_hld_lst_LR_full, file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(diff_train_lst_RF_full, file = sample)\n",
    "            print(diff_lst_RF_full, file = sample)\n",
    "            print(diff_hld_lst_RF_full, file = sample)\n",
    "            print('\\n', file = sample)              \n",
    "            \n",
    "            sample.close()\n",
    "    \n",
    "    mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR = np.mean(avgsc_train_lst_LR_full), np.mean(avgsc_lst_LR_full), np.mean(avgsc_hld_lst_LR_full)\n",
    "    mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF = np.mean(avgsc_train_lst_RF_full), np.mean(avgsc_lst_RF_full), np.mean(avgsc_hld_lst_RF_full)\n",
    "\n",
    "    return mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR, mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF    \n",
    "#     return avgsc_train_lst_LR_full, avgsc_lst_LR_full, avgsc_hld_lst_LR_full, avgsc_train_lst_RF_full, avgsc_lst_RF_full, avgsc_hld_lst_RF_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IntroduceDirtinessSkew(dfAblation, dataDownstream, attribute_names, dxr, PERC_OCCUR, scrtr_LR, scrval_LR, scrte_LR, scrtr_RF, scrval_RF, scrte_RF, mse_LR_TR, bias_LR_TR, var_LR_TR, mse_RF_TR, bias_RF_TR, var_RF_TR):\n",
    "    global SAVEPLACE, PERC_ENTY\n",
    "    categcols = ['duplicateColumn']\n",
    "    \n",
    "    ### p1_perc --> Percentage of entities that have duplicates\n",
    "    ### noise --> Percentage of occurences that are diluted with duplicate values    \n",
    "    \n",
    "    GRP_SIZE_MINUS_1 = 1\n",
    "    PERC_OCCUR = PERC_OCCUR*0.01\n",
    "    NUM_DIRTY_DT = 3\n",
    "\n",
    "    for curcol in categcols:\n",
    "        curdic = dict(dataDownstream[curcol].value_counts())\n",
    "\n",
    "        p1_perc = [PERC_ENTY]\n",
    "        for CURPERC in p1_perc:\n",
    "            print('CURPERC value is:' + str(CURPERC))\n",
    "            \n",
    "            tmp = ((dataDownstream[curcol].nunique())*CURPERC)*1.0/100\n",
    "            NENT = int(round(tmp,0))            \n",
    "\n",
    "            lst_vals = list(curdic.keys())\n",
    "            \n",
    "            print('Dictionary keys:')\n",
    "            print(lst_vals)            \n",
    "            \n",
    "            if CURPERC == 100:\n",
    "                NENT = dataDownstream[curcol].nunique()\n",
    "                possible_combinations = [random.sample(lst_vals,NENT)]\n",
    "            else:\n",
    "                possible_combinations = []\n",
    "                dic_of_strlsts = {}\n",
    "                \n",
    "                while True:\n",
    "                    if len(dic_of_strlsts) == NUM_DIRTY_DT: break\n",
    "                    tmplst = random.sample(lst_vals, NENT)\n",
    "\n",
    "                    if str(sorted(tmplst)) not in dic_of_strlsts:\n",
    "                        dic_of_strlsts[str(sorted(tmplst))] = 1\n",
    "                        possible_combinations.append(tmplst)     \n",
    "         \n",
    "                print('All possible_combinations:')\n",
    "                print(possible_combinations)  \n",
    "\n",
    "            print('NENT value is:' + str(NENT))\n",
    "\n",
    "            noise = PERC_OCCUR\n",
    "            indexlstlst = []\n",
    "\n",
    "            bestPerformingModel_LR_full, avgsc_train_lst_LR_full, avgsc_lst_LR_full, avgsc_hld_lst_LR_full = [],[],[],[]\n",
    "            bestPerformingModel_RF_full, avgsc_train_lst_RF_full, avgsc_lst_RF_full, avgsc_hld_lst_RF_full = [],[],[],[]\n",
    "            mse_LR_full, bias_LR_full, var_LR_full = [],[],[]\n",
    "            mse_RF_full, bias_RF_full, var_RF_full = [],[],[]            \n",
    "            \n",
    "            k = 0\n",
    "            for comb in possible_combinations:\n",
    "                k = k + 1\n",
    "                print(comb)\n",
    "                curdataDownstream = copy.deepcopy(dataDownstream)\n",
    "\n",
    "#                 GRP_SIZE_MINUS_1_lst = [1,1,1,1,1,2,2,2,3,3,4,5]\n",
    "                GRP_SIZE_MINUS_1_lst = SampleZipf(nr, dxr, NENT)\n",
    "                print('zipf lst:')\n",
    "                print(GRP_SIZE_MINUS_1_lst)\n",
    "                \n",
    "                for thisiter in range(len(comb)):\n",
    "                    pq = comb[thisiter]\n",
    "                    print('\\nCurrent value is:' + str(pq))\n",
    "\n",
    "                    noise = random.uniform(0, 0.50)\n",
    "                    dirtiness = int(curdic[pq]*noise)\n",
    "    #                     print(dirtiness)\n",
    "                    abc = curdataDownstream[curdataDownstream[curcol] == pq].sample(dirtiness, random_state=1)\n",
    "                    indxlst = abc.index.tolist()\n",
    "                    indexlstlst.append(indxlst)\n",
    "\n",
    "                    chk_indxlst = chunkIt(indxlst, GRP_SIZE_MINUS_1_lst[thisiter])\n",
    "\n",
    "                    for j in range(len(chk_indxlst)): curdataDownstream.loc[chk_indxlst[j], curcol] = str(pq) + '_' + str(j) + '_dummy'    \n",
    "            \n",
    "            \n",
    "                print(curdataDownstream[curcol].value_counts())\n",
    "                print('\\n')\n",
    "                           \n",
    "\n",
    "                bestPerformingModel_LR,avgsc_train_lst_LR,avgsc_lst_LR,avgsc_hld_lst_LR = LogRegClassifier(curdataDownstream, y, y_cur, attribute_names, 0)\n",
    "                bestPerformingModel_RF,avgsc_train_lst_RF,avgsc_lst_RF,avgsc_hld_lst_RF = RandForestClassifier(curdataDownstream, y, y_cur, attribute_names, 0)\n",
    "\n",
    "                mse_LR, bias_LR, var_LR, mse_RF, bias_RF, var_RF = BiasVarDecomp(curdataDownstream, y, bestPerformingModel_LR, bestPerformingModel_RF)\n",
    "        \n",
    "                avgsc_train_lst_LR,avgsc_lst_LR,avgsc_hld_lst_LR = round(avgsc_train_lst_LR*100.0,3), round(avgsc_lst_LR*100.0,3), round(avgsc_hld_lst_LR*100.0,3)\n",
    "                avgsc_train_lst_RF,avgsc_lst_RF,avgsc_hld_lst_RF = round(avgsc_train_lst_RF*100.0,3), round(avgsc_lst_RF*100.0,3), round(avgsc_hld_lst_RF*100.0,3)\n",
    "        \n",
    "                bestPerformingModel_LR_full.append(bestPerformingModel_LR)\n",
    "                avgsc_train_lst_LR_full.append(avgsc_train_lst_LR)\n",
    "                avgsc_lst_LR_full.append(avgsc_lst_LR)\n",
    "                avgsc_hld_lst_LR_full.append(avgsc_hld_lst_LR)\n",
    "\n",
    "                bestPerformingModel_RF_full.append(bestPerformingModel_RF)\n",
    "                avgsc_train_lst_RF_full.append(avgsc_train_lst_RF)\n",
    "                avgsc_lst_RF_full.append(avgsc_lst_RF)\n",
    "                avgsc_hld_lst_RF_full.append(avgsc_hld_lst_RF)\n",
    "                \n",
    "                mse_LR_full.append(mse_LR)\n",
    "                bias_LR_full.append(bias_LR)\n",
    "                var_LR_full.append(var_LR)\n",
    "                \n",
    "                mse_RF_full.append(mse_RF)\n",
    "                bias_RF_full.append(bias_RF)\n",
    "                var_RF_full.append(var_RF)\n",
    "            \n",
    "            diff_train_lst_LR_full = [round(scrtr_LR - number,3) for number in avgsc_train_lst_LR_full]\n",
    "            diff_lst_LR_full = [round(scrval_LR - number,3) for number in avgsc_lst_LR_full] \n",
    "            diff_hld_lst_LR_full = [round(scrte_LR - number,3) for number in avgsc_hld_lst_LR_full]\n",
    "            \n",
    "            diff_train_lst_RF_full = [round(scrtr_RF - number,3) for number in avgsc_train_lst_RF_full]\n",
    "            diff_lst_RF_full = [round(scrval_RF - number,3) for number in avgsc_lst_RF_full]\n",
    "            diff_hld_lst_RF_full = [round(scrte_RF - number,3) for number in avgsc_hld_lst_RF_full]\n",
    "\n",
    "            diff_mse_LR_full = [(number-mse_LR_TR) for number in mse_LR_full]\n",
    "            diff_bias_LR_full = [(number-bias_LR_TR) for number in bias_LR_full]\n",
    "            diff_var_LR_full = [(number-var_LR_TR) for number in var_LR_full]\n",
    "\n",
    "            diff_mse_RF_full = [(number-mse_RF_TR) for number in mse_RF_full]\n",
    "            diff_bias_RF_full = [(number-bias_RF_TR) for number in bias_RF_full]\n",
    "            diff_var_RF_full = [(number-var_RF_TR) for number in var_RF_full]            \n",
    "            \n",
    "            sample = open(SAVEPLACE, 'a')\n",
    "            print('Original Dataset:', file = sample)\n",
    "            print('---', file = sample)\n",
    "            print(scrtr_LR, file = sample)\n",
    "            print(scrval_LR, file = sample)\n",
    "            print(scrte_LR, file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(scrtr_RF, file = sample)\n",
    "            print(scrval_RF, file = sample)\n",
    "            print(scrte_RF, file = sample)\n",
    "#             print('\\n', file = sample)            \n",
    "            \n",
    "            print('---', file = sample)\n",
    "            print(dfAblation, file = sample)\n",
    "            print('\\n', file = sample)\n",
    "            \n",
    "            print('Original MSE,bias,var -> LR,RF', file = sample)\n",
    "            print(str(mse_LR_TR) + ' ' + str(bias_LR_TR) + ' ' + str(var_LR_TR), file = sample)\n",
    "            print(str(mse_RF_TR) + ' ' + str(bias_RF_TR) + ' ' + str(var_RF_TR), file = sample)\n",
    "            print('\\n', file = sample)            \n",
    "            \n",
    "            print('Categories that are diluted with duplicates:', file = sample)\n",
    "            print(possible_combinations[:10], file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(avgsc_train_lst_LR_full, file = sample)\n",
    "            print(avgsc_lst_LR_full, file = sample)\n",
    "            print(avgsc_hld_lst_LR_full, file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(avgsc_train_lst_RF_full, file = sample)\n",
    "            print(avgsc_lst_RF_full, file = sample)\n",
    "            print(avgsc_hld_lst_RF_full, file = sample)\n",
    "            print('\\n', file = sample)\n",
    "            \n",
    "            print('Difference between Orignal and Duplicate Dataset:', file = sample)\n",
    "            print('---', file = sample)\n",
    "            print(diff_train_lst_LR_full, file = sample)\n",
    "            print(diff_lst_LR_full, file = sample)\n",
    "            print(diff_hld_lst_LR_full, file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(diff_train_lst_RF_full, file = sample)\n",
    "            print(diff_lst_RF_full, file = sample)\n",
    "            print(diff_hld_lst_RF_full, file = sample)\n",
    "            print('\\n', file = sample)              \n",
    "            \n",
    "            print('MSE,bias,var --- LR', file = sample)\n",
    "            print(mse_LR_full, file = sample)\n",
    "            print(bias_LR_full, file = sample)\n",
    "            print(var_LR_full, file = sample)\n",
    "            print('\\n', file = sample)       \n",
    "            \n",
    "            print('MSE,bias,var --- RF', file = sample)\n",
    "            print(mse_RF_full, file = sample)\n",
    "            print(bias_RF_full, file = sample)\n",
    "            print(var_RF_full, file = sample)\n",
    "            print('\\n', file = sample)  \n",
    "            \n",
    "            print('Difference MSE,bias,var --- LR', file = sample)\n",
    "            print(diff_mse_LR_full, file = sample)\n",
    "            print(diff_bias_LR_full, file = sample)\n",
    "            print(diff_var_LR_full, file = sample)\n",
    "            print('\\n', file = sample)    \n",
    "            \n",
    "            print('Difference MSE,bias,var --- RF', file = sample)\n",
    "            print(diff_mse_RF_full, file = sample)\n",
    "            print(diff_bias_RF_full, file = sample)\n",
    "            print(diff_var_RF_full, file = sample)\n",
    "            print('\\n', file = sample)              \n",
    "            \n",
    "            sample.close()\n",
    "    \n",
    "    mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR = np.mean(avgsc_train_lst_LR_full), np.mean(avgsc_lst_LR_full), np.mean(avgsc_hld_lst_LR_full)\n",
    "    mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF = np.mean(avgsc_train_lst_RF_full), np.mean(avgsc_lst_RF_full), np.mean(avgsc_hld_lst_RF_full)\n",
    "\n",
    "    return mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR, mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF    \n",
    "#     return avgsc_train_lst_LR_full, avgsc_lst_LR_full, avgsc_hld_lst_LR_full, avgsc_train_lst_RF_full, avgsc_lst_RF_full, avgsc_hld_lst_RF_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IntroduceDirtinessH2o(dfAblation, dataDownstream, attribute_names, dxr, PERC_OCCUR, scrtr_LR, scrval_LR, scrte_LR, scrtr_RF, scrval_RF, scrte_RF):\n",
    "    global SAVEPLACE, PERC_ENTY\n",
    "    categcols = ['duplicateColumn']\n",
    "    \n",
    "    ### p1_perc --> Percentage of entities that have duplicates\n",
    "    ### noise --> Percentage of occurences that are diluted with duplicate values    \n",
    "    \n",
    "    GRP_SIZE_MINUS_1 = 1\n",
    "    PERC_OCCUR = PERC_OCCUR*0.01\n",
    "    NUM_DIRTY_DT = 10\n",
    "\n",
    "    for curcol in categcols:\n",
    "        curdic = dict(dataDownstream[curcol].value_counts())\n",
    "\n",
    "        p1_perc = [PERC_ENTY]\n",
    "        for CURPERC in p1_perc:\n",
    "            print('CURPERC value is:' + str(CURPERC))\n",
    "            \n",
    "            tmp = ((dataDownstream[curcol].nunique())*CURPERC)*1.0/100\n",
    "            NENT = int(round(tmp,0))            \n",
    "\n",
    "            lst_vals = list(curdic.keys())\n",
    "            \n",
    "            print('Dictionary keys:')\n",
    "            print(lst_vals)            \n",
    "            \n",
    "            if CURPERC == 100:\n",
    "                NENT = dataDownstream[curcol].nunique()\n",
    "                possible_combinations = [random.sample(lst_vals,NENT)]\n",
    "            else:\n",
    "                possible_combinations = []\n",
    "                dic_of_strlsts = {}\n",
    "                \n",
    "                while True:\n",
    "                    if len(dic_of_strlsts) == NUM_DIRTY_DT: break\n",
    "                    tmplst = random.sample(lst_vals, NENT)\n",
    "\n",
    "                    if str(sorted(tmplst)) not in dic_of_strlsts:\n",
    "                        dic_of_strlsts[str(sorted(tmplst))] = 1\n",
    "                        possible_combinations.append(tmplst)     \n",
    "         \n",
    "                print('All possible_combinations:')\n",
    "                print(possible_combinations)  \n",
    "                  \n",
    "\n",
    "            print('NENT value is:' + str(NENT))\n",
    "\n",
    "            noise = PERC_OCCUR\n",
    "            indexlstlst = []\n",
    "\n",
    "            bestPerformingModel_RF_full, avgsc_train_lst_RF_full, avgsc_lst_RF_full, avgsc_hld_lst_RF_full = [],[],[],[]\n",
    "\n",
    "            k = 0\n",
    "            for comb in possible_combinations:\n",
    "#                 if k > 1: break\n",
    "                k = k + 1\n",
    "                print(comb)\n",
    "                curdataDownstream = copy.deepcopy(dataDownstream)\n",
    "\n",
    "                for pq in comb:\n",
    "#                     print('\\nCurrent value is:' + str(pq))\n",
    "    \n",
    "                    dirtiness = int(curdic[pq]*noise)\n",
    "#                     print(dirtiness)\n",
    "                    abc = curdataDownstream[curdataDownstream[curcol] == pq].sample(dirtiness, random_state=1)\n",
    "                    indxlst = abc.index.tolist()\n",
    "                    indexlstlst.append(indxlst)\n",
    "\n",
    "                    chk_indxlst = chunkIt(indxlst, GRP_SIZE_MINUS_1)\n",
    "\n",
    "                    for j in range(len(chk_indxlst)): curdataDownstream.loc[chk_indxlst[j], curcol] = str(pq) + '_' + str(j) + '_dummy'    \n",
    "\n",
    "                print(curdataDownstream[curcol].value_counts())\n",
    "                print('\\n')\n",
    "\n",
    "                bestPerformingModel_RF,avgsc_train_lst_RF,avgsc_lst_RF,avgsc_hld_lst_RF = RandForestH2oClassifier(curdataDownstream, y, y_cur, attribute_names, 'y', 0)\n",
    "        \n",
    "                avgsc_train_lst_RF,avgsc_lst_RF,avgsc_hld_lst_RF = round(avgsc_train_lst_RF*100.0,3), round(avgsc_lst_RF*100.0,3), round(avgsc_hld_lst_RF*100.0,3)\n",
    "\n",
    "                bestPerformingModel_RF_full.append(bestPerformingModel_RF)\n",
    "                avgsc_train_lst_RF_full.append(avgsc_train_lst_RF)\n",
    "                avgsc_lst_RF_full.append(avgsc_lst_RF)\n",
    "                avgsc_hld_lst_RF_full.append(avgsc_hld_lst_RF)\n",
    "\n",
    "            \n",
    "            diff_train_lst_RF_full = [round(scrtr_RF - number,3) for number in avgsc_train_lst_RF_full]\n",
    "            diff_lst_RF_full = [round(scrval_RF - number,3) for number in avgsc_lst_RF_full]\n",
    "            diff_hld_lst_RF_full = [round(scrte_RF - number,3) for number in avgsc_hld_lst_RF_full]\n",
    "\n",
    "            sample = open(SAVEPLACE, 'a')\n",
    "            print('Original Dataset:', file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(scrtr_RF, file = sample)\n",
    "            print(scrval_RF, file = sample)\n",
    "            print(scrte_RF, file = sample)\n",
    "#             print('\\n', file = sample)            \n",
    "            \n",
    "            print('---', file = sample)            \n",
    "            print(dfAblation, file = sample)\n",
    "            print('\\n', file = sample)             \n",
    "            \n",
    "            print('Categories that are diluted with duplicates:', file = sample)\n",
    "            print(possible_combinations[:10], file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(avgsc_train_lst_RF_full, file = sample)\n",
    "            print(avgsc_lst_RF_full, file = sample)\n",
    "            print(avgsc_hld_lst_RF_full, file = sample)\n",
    "            print('\\n', file = sample)\n",
    "            \n",
    "            print('Difference between Orignal and Duplicate Dataset:', file = sample)\n",
    "\n",
    "            print('---', file = sample)\n",
    "            print(diff_train_lst_RF_full, file = sample)\n",
    "            print(diff_lst_RF_full, file = sample)\n",
    "            print(diff_hld_lst_RF_full, file = sample)\n",
    "            print('\\n', file = sample)              \n",
    "            \n",
    "            sample.close()\n",
    "\n",
    "    mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF = np.mean(avgsc_train_lst_RF_full), np.mean(avgsc_lst_RF_full), np.mean(avgsc_hld_lst_RF_full)\n",
    "\n",
    "    return mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR, mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF    \n",
    "#     return avgsc_train_lst_LR_full, avgsc_lst_LR_full, avgsc_hld_lst_LR_full, avgsc_train_lst_RF_full, avgsc_lst_RF_full, avgsc_hld_lst_RF_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mysampler(domsize, nr):\n",
    "    global dr\n",
    "    num_per_dom = int(nr/domsize)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    arr,arr2 = [],[]\n",
    "    for i in range(domsize):\n",
    "        for j in range(num_per_dom): \n",
    "            arr.append(i)\n",
    "            arr2.append(np.random.normal(0,1,1).tolist()[0])\n",
    "            \n",
    "    curlen = len(arr)\n",
    "    \n",
    "    j=0\n",
    "    while j < (nr-curlen):\n",
    "        arr.append(j)\n",
    "        j += 1\n",
    "             \n",
    "        \n",
    "    for k in range(dr):\n",
    "        random.shuffle(arr)        \n",
    "        df[k] = arr\n",
    "    \n",
    "#     print(df)\n",
    "        \n",
    "    rutplst = df.values.tolist()\n",
    "    return rutplst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SAVEPLACE = 'logs/synthetic/allx/duplicates-1(dr=4)/Occurrence=' + str(PERC_OCCUR) + '/duplicateColumn(dxr=' + str(dxrlst[0]) + ')(nr=' + str(trE) + ')-' + str(PERC_ENTY) + '.txt'\n",
    "sample = open(SAVEPLACE, 'w')\n",
    "\n",
    "train_errors_lst, test_errors_lst, train_errors_lst_Categ, test_errors_lst_Categ, train_errors_lst_both, test_errors_lst_both = [],[],[],[],[],[]\n",
    "Log_train_errors_lst, Log_test_errors_lst, Log_train_errors_lst_Categ, Log_test_errors_lst_Categ, Log_train_errors_lst_both, Log_test_errors_lst_both = [],[],[],[],[],[]\n",
    "val_errors_lst, val_errors_lst_Categ, val_errors_lst_both, Log_val_errors_lst, Log_val_errors_lst_Categ, Log_val_errors_lst_both = [],[],[],[],[],[]\n",
    "avg_depth_numeric_lst,avg_depth_categ_lst,avg_depth_both_lst = [],[],[]\n",
    "bestPerformingModels_Categ, bestPerformingModels_Numeric, bestPerformingModels_both = [],[],[]\n",
    "Log_avgruntime_lst,Log_avgruntime_Categ_lst,Log_avgruntime_both_lst,avgruntime_lst,avgruntime_Categ_lst,avgruntime_both_lst = [],[],[],[],[],[]\n",
    "\n",
    "# for KNINT in nonintcategvarslst:\n",
    "for dxr in dxrlst:\n",
    "#     ProbLst = createCPT(dxr)\n",
    "    DomainLst = list(range(dxr))\n",
    "    OHElst = {}\n",
    "    j=0\n",
    "    for x in DomainLst:\n",
    "        lst = [0] * (len(DomainLst))\n",
    "        lst[j] = 1\n",
    "        j += 1\n",
    "        OHElst[x] = lst\n",
    "\n",
    "\n",
    "    CPTdic = CreateNDCPT(dr,dxr)   \n",
    "    ########################################################################################################\n",
    "\n",
    "    avgtestloss, avgtrainloss, avgtestloss_Categ, avgtrainloss_Categ, avgtestloss_both, avgtrainloss_both  = 0.0,0.0,0.0,0.0,0.0,0.0\n",
    "    Log_avgvalloss , Log_avgvalloss_Categ, Log_avgvalloss_both, avgvalloss, avgvalloss_Categ, avgvalloss_both  = 0.0,0.0,0.0,0.0,0.0,0.0\n",
    "    Log_avgtestloss , Log_avgtrainloss , Log_avgtestloss_Categ, Log_avgtrainloss_Categ, Log_avgtestloss_both, Log_avgtrainloss_both = 0.0,0.0,0.0,0.0,0.0,0.0\n",
    "    avg_depth_numeric, avg_depth_categ, avg_depth_both = 0.0,0.0,0.0\n",
    "    Log_avgruntime,Log_avgruntime_Categ,Log_avgruntime_both,avgruntime,avgruntime_Categ,avgruntime_both = 0.0,0.0,0.0,0.0,0.0,0.0\n",
    "    first_LR_lst, first_RF_lst, second_LR_lst, second_RF_lst, third_LR_lst, third_RF_lst = [],[],[],[],[],[]\n",
    "\n",
    "    for ti in range(numD):\n",
    "        print('START#############################################################################################')\n",
    "\n",
    "        rtuplst = mysampler(dxr, nr)\n",
    "        rtuples = {}\n",
    "        for rid in range(nr):  rtuples[rid] = rtuplst[rid]\n",
    "\n",
    "    #############################################################################################\n",
    "    ### Full DataSet\n",
    "    #############################################################################################    \n",
    "        fullsetx,fullsetx_Categ,fullsetx_both,fullsety = [],[],[],[]\n",
    "        for dataid in range(int(nr)):\n",
    "            fullsetx, fullsetx_Categ, fullsetx_both, fullsety = buildDataset(fullsetx, fullsetx_Categ, fullsetx_both, fullsety, rtuples[dataid],DomainLst,OHElst,dxr)\n",
    "#         print(fullsetx_Categ)\n",
    "\n",
    "        print('-------')        \n",
    "        print('% 0 Entries:')\n",
    "        print(fullsety.count(0)*100/len(fullsety))        \n",
    "        print('-------')        \n",
    "\n",
    "        dataDownstream, y, y_cur, attribute_names = DatasetToDataFrame(fullsetx_Categ, fullsety)\n",
    "#         y_cur = [10,1,1]\n",
    "        print('#############################################################################################')\n",
    "        print('Mutual Info:')\n",
    "        print(sklearn.feature_selection.mutual_info_classif(dataDownstream, y, discrete_features = True))\n",
    "        print('#############################################################################################')\n",
    "#         print(dataDownstream, y, y_cur, attribute_names)\n",
    "    #############################################################################################\n",
    "    ### Original LR and RF\n",
    "    #############################################################################################\n",
    "        bestPerformingModel_LR,scrtr_LR, scrval_LR, scrte_LR = LogRegClassifier(dataDownstream, y, y_cur, attribute_names, 0)\n",
    "        bestPerformingModel_RF,scrtr_RF, scrval_RF, scrte_RF = RandForestClassifier(dataDownstream, y, y_cur, attribute_names, 0)\n",
    "\n",
    "\n",
    "        scrtr_LR, scrval_LR, scrte_LR = round(scrtr_LR*100.0,3), round(scrval_LR*100.0,3), round(scrte_LR*100.0,3)\n",
    "        scrtr_RF, scrval_RF, scrte_RF = round(scrtr_RF*100.0,3), round(scrval_RF*100.0,3), round(scrte_RF*100.0,3)\n",
    "        print('#############################################################################################')\n",
    "        print('Current Datset #:' + str(ti))\n",
    "        print('Original LR and RF:')\n",
    "        print(scrtr_LR, scrval_LR, scrte_LR)\n",
    "        print(scrtr_RF, scrval_RF, scrte_RF)\n",
    "        print('#############################################################################################')        \n",
    "\n",
    "        Log_avgtrainloss += scrtr_LR\n",
    "        Log_avgvalloss += scrval_LR\n",
    "        Log_avgtestloss += scrte_LR\n",
    "        Log_avgruntime += 0\n",
    "\n",
    "        avgtrainloss += scrtr_RF\n",
    "        avgvalloss += scrval_RF\n",
    "        avgtestloss += scrte_RF\n",
    "        avgruntime += 0\n",
    "\n",
    "        dfAblation,LR_index,RF_index = AblationStudy(dataDownstream, y, y_cur, attribute_names, scrtr_LR, scrval_LR, scrte_LR, scrtr_RF, scrval_RF, scrte_RF)\n",
    "#         first_LR_lst.append(LR_index[0]),second_LR_lst.append(LR_index[1]),third_LR_lst.append(LR_index[2])\n",
    "#         first_RF_lst.append(RF_index[0]),second_RF_lst.append(RF_index[1]),third_RF_lst.append(RF_index[2])        \n",
    "        print('#############################################################################################')\n",
    "        print('Ablation:')\n",
    "        print(dfAblation)\n",
    "        print('#############################################################################################')\n",
    "\n",
    "    ############################################################################################\n",
    "    ## Duplicates LR and RF\n",
    "    ############################################################################################\n",
    "        if skewPresent: mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR, mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF = IntroduceDirtinessSkew(dfAblation, dataDownstream, attribute_names, dxr, PERC_OCCUR, scrtr_LR, scrval_LR, scrte_LR, scrtr_RF, scrval_RF, scrte_RF)\n",
    "        else: mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR, mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF = IntroduceDirtiness(dfAblation, dataDownstream, attribute_names, dxr, PERC_OCCUR, scrtr_LR, scrval_LR, scrte_LR, scrtr_RF, scrval_RF, scrte_RF)\n",
    "\n",
    "        Log_avgtrainloss_Categ += mean_train_acc_LR\n",
    "        Log_avgvalloss_Categ += mean_val_acc_LR\n",
    "        Log_avgtestloss_Categ += mean_test_acc_LR\n",
    "        Log_avgruntime_Categ += 0\n",
    "\n",
    "        avgtrainloss_Categ += mean_train_acc_RF\n",
    "        avgvalloss_Categ += mean_val_acc_RF\n",
    "        avgtestloss_Categ += mean_test_acc_RF\n",
    "        avgruntime_Categ += 0        \n",
    "\n",
    "        print('#############################################################################################')        \n",
    "        print('Current Datset #:' + str(ti))\n",
    "        print('Duplicate LR and RF:')\n",
    "\n",
    "        print(mean_train_acc_LR, mean_val_acc_LR, mean_test_acc_LR)\n",
    "        print(mean_train_acc_RF, mean_val_acc_RF, mean_test_acc_RF)\n",
    "        print('#############################################################################################')        \n",
    "\n",
    "    print('\\n Domain size:' + str(dxr))\n",
    "    print('Logistic Regression:')\n",
    "    print('Original Accuracy:')\n",
    "    (Log_avgtrainloss, Log_avgvalloss, Log_avgtestloss, Log_avgruntime) = (float(Log_avgtrainloss)/numD, float(Log_avgvalloss)/numD, float(Log_avgtestloss)/numD, float(Log_avgruntime)/numD)\n",
    "    print(Log_avgtrainloss, Log_avgvalloss, Log_avgtestloss, Log_avgruntime)\n",
    "\n",
    "    Log_train_errors_lst.append(Log_avgtrainloss)\n",
    "    Log_val_errors_lst.append(Log_avgvalloss)\n",
    "    Log_test_errors_lst.append(Log_avgtestloss)\n",
    "    Log_avgruntime_lst.append(Log_avgruntime)\n",
    "\n",
    "    print('Duplicate Accuracy:')\n",
    "    (Log_avgtrainloss_Categ, Log_avgvalloss_Categ , Log_avgtestloss_Categ, Log_avgruntime_Categ ) = (float(Log_avgtrainloss_Categ)/numD, float(Log_avgvalloss_Categ)/numD, float(Log_avgtestloss_Categ)/numD, float(Log_avgruntime_Categ)/numD)\n",
    "    print(Log_avgtrainloss_Categ, Log_avgvalloss_Categ , Log_avgtestloss_Categ, Log_avgruntime_Categ)\n",
    "\n",
    "    Log_train_errors_lst_Categ.append(Log_avgtrainloss_Categ)\n",
    "    Log_val_errors_lst_Categ.append(Log_avgvalloss_Categ)    \n",
    "    Log_test_errors_lst_Categ.append(Log_avgtestloss_Categ)    \n",
    "    Log_avgruntime_Categ_lst.append(Log_avgruntime_Categ)\n",
    "\n",
    "    print('END#############################################################################################')\n",
    "\n",
    "    print('Random Forest:')\n",
    "    print('Original Accuracy:')\n",
    "    (avgtrainloss, avgvalloss, avgtestloss,avg_depth_numeric, avgruntime) = (float(avgtrainloss)/numD, float(avgvalloss)/numD, float(avgtestloss)/numD, float(avg_depth_numeric)/numD, float(avgruntime)/numD)\n",
    "    print(avgtrainloss, avgvalloss, avgtestloss,avg_depth_numeric, avgruntime)\n",
    "\n",
    "    train_errors_lst.append(avgtrainloss)\n",
    "    val_errors_lst.append(avgvalloss)    \n",
    "    test_errors_lst.append(avgtestloss)\n",
    "    avg_depth_numeric_lst.append(avg_depth_numeric)\n",
    "    avgruntime_lst.append(avgruntime)\n",
    "\n",
    "    print('Duplicate Accuracy:')\n",
    "    (avgtrainloss_Categ, avgvalloss_Categ , avgtestloss_Categ, avg_depth_categ, avgruntime_Categ ) = (float(avgtrainloss_Categ)/numD, float(avgvalloss_Categ)/numD, float(avgtestloss_Categ)/numD, float(avg_depth_categ)/numD, float(avgruntime_Categ)/numD )\n",
    "    print(avgtrainloss_Categ, avgvalloss_Categ , avgtestloss_Categ, avg_depth_categ, avgruntime_Categ)\n",
    "\n",
    "    train_errors_lst_Categ.append(avgtrainloss_Categ)\n",
    "    val_errors_lst_Categ.append(avgvalloss_Categ)\n",
    "    test_errors_lst_Categ.append(avgtestloss_Categ)    \n",
    "    avg_depth_categ_lst.append(avg_depth_categ)\n",
    "    avgruntime_Categ_lst.append(avgruntime_Categ)\n",
    "\n",
    "    print('Both Accuracy:')\n",
    "    (avgtrainloss_both, avgvalloss_both , avgtestloss_both, avg_depth_both, avgruntime_both ) = (float(avgtrainloss_both)/numD, float(avgvalloss_both)/numD, float(avgtestloss_both)/numD, float(avg_depth_both)/numD, float(avgruntime_both)/numD )\n",
    "    print(avgtrainloss_both, avgvalloss_both , avgtestloss_both, avg_depth_both, avgruntime_both)\n",
    "\n",
    "    train_errors_lst_both.append(avgtrainloss_both)\n",
    "    val_errors_lst_both.append(avgvalloss_both)\n",
    "    test_errors_lst_both.append(avgtestloss_both)    \n",
    "    avg_depth_both_lst.append(avg_depth_both)    \n",
    "    avgruntime_both_lst.append(avgruntime_both)\n",
    "\n",
    "    print('END#############################################################################################')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
