{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Load_Predictions import *\n",
    "from downstream_model import *\n",
    "from Featurize import *\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import copy\n",
    "import warnings\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import math\n",
    "import scipy\n",
    "import time\n",
    "import h2o\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Specify the input file name, feature type annotations, and the name of the duplicate and target column\n",
    "\n",
    "InputFilePath = 'Etailing/etailing.csv'\n",
    "LabelFile = 'labels/dataFeaturized-etailing.csv'\n",
    "\n",
    "duplicateColumn = 'State'\n",
    "TargetColumn = 'What is the maximum cart value you ever shopped?'\n",
    "\n",
    "### when using similarity encoding, just set the flag to 1. 0 means one-hot encoding.\n",
    "similarity_flag = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDownstream = pd.read_csv(InputFilePath)\n",
    "dataDownstream = dataDownstream.sample(frac=1, random_state=100)\n",
    "\n",
    "dataDownstream = dataDownstream[dataDownstream[TargetColumn].notna()]\n",
    "dataDownstream = dataDownstream[dataDownstream[duplicateColumn].notna()]\n",
    "\n",
    "dataDownstream = dataDownstream.fillna('0')\n",
    "dataDownstream = dataDownstream.reset_index(drop=True)\n",
    "\n",
    "y = dataDownstream[[TargetColumn]]\n",
    "dataDownstream = dataDownstream.drop(TargetColumn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFeaturized = FeaturizeFile(dataDownstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_names = dataDownstream.columns.values.tolist()\n",
    "\n",
    "attribute_dic = {}\n",
    "labelsfile = pd.read_csv()\n",
    "\n",
    "for index,row in labelsfile.iterrows(): attribute_dic[row['Attribute_name']] = row['label']\n",
    "attribute_dic[duplicateColumn] = 10\n",
    "print(attribute_dic)\n",
    "\n",
    "y_cur = []\n",
    "for x in attribute_names: y_cur.append(attribute_dic[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate One-hot Encoding\n",
    "lrm,avgsc_train_lst_LR,avgsc_lst_LR,avgsc_hld_lst_LR = LogRegClassifier(dataDownstream,y,y_cur,attribute_names,similarity_flag)\n",
    "rfm,avgsc_train_lst_RF,avgsc_lst_RF,avgsc_hld_lst_RF = RandForestClassifier(dataDownstream,y,y_cur,attribute_names,similarity_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDownstream_h2o = copy.deepcopy(dataDownstream)\n",
    "y_h2o = copy.deepcopy(y)\n",
    "y_h2o.update('\"' + y_h2o.astype(str) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate String Encoding\n",
    "rfm_h2o, avgsc_train_lst_h2o,avgsc_lst_h2o,avgsc_hld_lst_h2o =  RandForestH2oClassifier(dataDownstream_h2o, y_h2o, y_cur,attribute_names,TargetColumn,similarity_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Deduped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(InputDownPath)\n",
    "df = df.sort_values(by=['times_entered'], ascending=False)\n",
    "df = df.sort_values(by=['group'])\n",
    "df = df.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prvgrpno = 0\n",
    "dicdups = {}\n",
    "curdup = ''\n",
    "for index, row in df.iterrows():\n",
    "    curgrpno = row['group']\n",
    "    if curgrpno!= curgrpno:\n",
    "        curdup = row[duplicateColumn]\n",
    "        dicdups[curdup] = prvgrpno\n",
    "        prvgrpno = prvgrpno + 1\n",
    "    else:\n",
    "        if prvgrpno != curgrpno: \n",
    "            curdup = row[duplicateColumn]\n",
    "            dicdups[curdup] = curdup\n",
    "        else: dicdups[row[duplicateColumn]] = curdup\n",
    "        prvgrpno = curgrpno\n",
    "\n",
    "print(len(dicdups))\n",
    "print(len(set(dicdups.values())))\n",
    "print(dicdups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDownstream_dedup = copy.deepcopy(dataDownstream)\n",
    "dataDownstream_dedup[duplicateColumn] = dataDownstream_dedup[duplicateColumn].fillna('0')\n",
    "\n",
    "CleanedduplicateColumn = 'Duplicate_CleanCol'\n",
    "dataDownstream_both = copy.deepcopy(dataDownstream)\n",
    "dataDownstream_both[CleanedduplicateColumn] = dataDownstream_both[duplicateColumn]\n",
    "dataDownstream_both[CleanedduplicateColumn] = dataDownstream_both[CleanedduplicateColumn].fillna('0')\n",
    "\n",
    "def func(x):\n",
    "    if x == '0': return '0'\n",
    "    try: return dicdups[x]\n",
    "    except KeyError: return '0'\n",
    "    \n",
    "dataDownstream_dedup[duplicateColumn] = dataDownstream_dedup[duplicateColumn].apply(lambda x: func(x))\n",
    "dataDownstream_both[CleanedduplicateColumn] = dataDownstream_both[CleanedduplicateColumn].apply(lambda x: func(x))\n",
    "\n",
    "dataFeaturized_dedup = FeaturizeFile(dataDownstream_dedup)\n",
    "dataFeaturized_both = FeaturizeFile(dataDownstream_both)\n",
    "\n",
    "attribute_names = dataDownstream_dedup.columns.values.tolist()\n",
    "attribute_names_both = dataDownstream_both.columns.values.tolist()\n",
    "\n",
    "y_cur_both = copy.deepcopy(y_cur)\n",
    "y_cur_both.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrm_dedup,avgsc_train_lst_LR_dedup,avgsc_lst_LR_dedup,avgsc_hld_lst_LR_dedup = LogRegClassifier(dataDownstream_dedup,y,y_cur,attribute_names,similarity_flag)\n",
    "rfm_dedup,avgsc_train_lst_RF_dedup,avgsc_lst_RF_dedup,avgsc_hld_lst_RF_dedup = RandForestClassifier(dataDownstream_dedup,y,y_cur,attribute_names,similarity_flag)\n",
    "bestPerformingModel, avgsc_train_lst,avgsc_lst,avgsc_hld_lst = RandForestH2oClassifier(dataDownstream_dedup, y_h2o, y_cur,attribute_names,TargetColumn,similarity_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
